{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Papers/summary.xml\", \"r\")\n",
    "soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paper_db>\n",
       "<paper>\n",
       "<paper_name>Get to the Point: Pointer-Generator Networks</paper_name>\n",
       "<task>Abstractive Summarisation</task>\n",
       "<data>Summarisation and Coverage Dataset</data>\n",
       "<contribution>Improved performance on OOVs coverage</contribution>\n",
       "<arch>LSTM Encoder-Decoder with Attention and Copy/Pointer</arch>\n",
       "<improv>Use on Transformers</improv>\n",
       "</paper>\n",
       "<paper>\n",
       "<paper_name>Data-to-Text Generation with Content Selection and Planning</paper_name>\n",
       "<task>Table to Text Generation, Content Selection and Planning</task>\n",
       "<data>RotoWire Dataset</data>\n",
       "<contribution>Planning step withour sacrificing end-to-end training</contribution>\n",
       "<arch>LSTM Encoder-Decoder with attention and copy mechanism. Two steps encoding - content selection and planning.</arch>\n",
       "<improv>Use it with Transformers</improv>\n",
       "</paper>\n",
       "<paper>\n",
       "<paper_name>Few-Shot NLG with Pre-Trained Language Models</paper_name>\n",
       "<task>JSON to Text Generation</task>\n",
       "<data>WikiBio</data>\n",
       "<contribution>Introduced Transfer Learning for generation phase, works really well with small data to start with.</contribution>\n",
       "<arch>LSTM Encoder with GPT-2 Decoder, combined with attention and copy mechanism.</arch>\n",
       "<improv>Try with Transformer Encoder</improv>\n",
       "</paper>\n",
       "</paper_db>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(\"<b></b>\")\n",
    "# original_tag = soup.b\n",
    "# original_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_tag = soup.new_tag(\"a\", href=\"http://www.example.com\")\n",
    "# original_tag.append(new_tag)\n",
    "# original_tag\n",
    "# # <b><a href=\"http://www.example.com\"></a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_tag.string = \"Link text.\"\n",
    "# original_tag\n",
    "# # <b><a href=\"http://www.example.com\">Link text.</a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tag = soup.new_tag(\"paper\")\n",
    "# new_tag.append(soup.new_tag(\"paper_name\"))\n",
    "# new_tag.append(soup.new_tag(\"summ\"))\n",
    "# new_tag.summ.string = \"d2t\"\n",
    "# new_tag.paper_name.string = \"ashish\"\n",
    "# original_tag.append(new_tag)\n",
    "# original_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A Hierarchical Model for Data-to-Text Generation\n",
      "2. A Mixed Hierarchical Attention based Encoder-Decoder Approach for Standard Table Summarisation\n",
      "3. Bridging the Structural Gap Between Encoding and Decoding for D2T Gen\n",
      "4. Can Neural Generators for Dialogue\n",
      "5. Challenges in Data-to-Document Generation\n",
      "6. Data-to-Text Generation with Content Selection and Planning\n",
      "7. Data-to-text Generation with Entity Modeling\n",
      "8. Deep Graph Convolutional Encoders for D2T Gen\n",
      "9. Deep Learning Approaches to Text Production\n",
      "10. Enhancing Neural Data-To-Text Generation Models with External Background Knowledge\n",
      "11. Few-Shot NLG with Pre-Trained Language Model\n",
      "12. Generating Descriptions from Structured Data Using a Bifocal Attention and Gated Orthogonalisation\n",
      "13. Generating Paraphrases from DBPedia using Deep Learning\n",
      "14. Key Fact as Pivot A Two-Stage Model for Low Resource Tab-to-Text Gen\n",
      "15. Neural data-to-text generation Comparison Between Pipeline and E2E\n",
      "16. Reference-Aware Language Models\n",
      "17. Semi-Supervised Neural Text Generation by Joint Learning of NLU and NLG\n",
      "18. Sequence-to-Sequence Models for Data-to-Text Natural Language Generation - Word v Char and Out Diversity\n",
      "19. Summarization with Pointer-Generator Networks\n",
      "20. Table-to-Text Generation by Structure-Aware Seq2seq Learning\n",
      "21. TabNet Attentive Interpretable Tabular Learning\n",
      "22. Text-to-Text Pre-Training for Data-to-Text Tasks\n",
      "23. The E2E Dataset New Challenges For End-to-End Generation\n",
      "24. ToTTo A Controlled Table-To-Text Generation Dataset\n"
     ]
    }
   ],
   "source": [
    "for i, paper in enumerate(os.listdir(\"../Papers/D2T/dl/\")):\n",
    "    file_name = (paper.split(\".\")[0]).strip()\n",
    "    print(\"{}. {}\".format(i+1, file_name))\n",
    "    new_tag = soup.new_tag(\"paper\")\n",
    "    new_tag.append(soup.new_tag(\"paper_name\"))\n",
    "    new_tag.append(soup.new_tag(\"task\"))\n",
    "    new_tag.append(soup.new_tag(\"data\"))\n",
    "    new_tag.append(soup.new_tag(\"contribution\"))\n",
    "    new_tag.append(soup.new_tag(\"arch\"))\n",
    "    new_tag.append(soup.new_tag(\"improv\"))\n",
    "    new_tag.paper_name.string = file_name\n",
    "    soup.append(new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paper_db>\n",
       "<paper>\n",
       "<paper_name>Get to the Point: Pointer-Generator Networks</paper_name>\n",
       "<task>Abstractive Summarisation</task>\n",
       "<data>Summarisation and Coverage Dataset</data>\n",
       "<contribution>Improved performance on OOVs coverage</contribution>\n",
       "<arch>LSTM Encoder-Decoder with Attention and Copy/Pointer</arch>\n",
       "<improv>Use on Transformers</improv>\n",
       "</paper>\n",
       "<paper>\n",
       "<paper_name>Data-to-Text Generation with Content Selection and Planning</paper_name>\n",
       "<task>Table to Text Generation, Content Selection and Planning</task>\n",
       "<data>RotoWire Dataset</data>\n",
       "<contribution>Planning step withour sacrificing end-to-end training</contribution>\n",
       "<arch>LSTM Encoder-Decoder with attention and copy mechanism. Two steps encoding - content selection and planning.</arch>\n",
       "<improv>Use it with Transformers</improv>\n",
       "</paper>\n",
       "<paper>\n",
       "<paper_name>Few-Shot NLG with Pre-Trained Language Models</paper_name>\n",
       "<task>JSON to Text Generation</task>\n",
       "<data>WikiBio</data>\n",
       "<contribution>Introduced Transfer Learning for generation phase, works really well with small data to start with.</contribution>\n",
       "<arch>LSTM Encoder with GPT-2 Decoder, combined with attention and copy mechanism.</arch>\n",
       "<improv>Try with Transformer Encoder</improv>\n",
       "</paper>\n",
       "</paper_db><paper><paper_name>A Hierarchical Model for Data-to-Text Generation</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>A Mixed Hierarchical Attention based Encoder-Decoder Approach for Standard Table Summarisation</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Bridging the Structural Gap Between Encoding and Decoding for D2T Gen</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Can Neural Generators for Dialogue</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Challenges in Data-to-Document Generation</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Data-to-Text Generation with Content Selection and Planning</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Data-to-text Generation with Entity Modeling</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Deep Graph Convolutional Encoders for D2T Gen</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Deep Learning Approaches to Text Production</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Enhancing Neural Data-To-Text Generation Models with External Background Knowledge</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Few-Shot NLG with Pre-Trained Language Model</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Generating Descriptions from Structured Data Using a Bifocal Attention and Gated Orthogonalisation</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Generating Paraphrases from DBPedia using Deep Learning</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Key Fact as Pivot A Two-Stage Model for Low Resource Tab-to-Text Gen</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Neural data-to-text generation Comparison Between Pipeline and E2E</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Reference-Aware Language Models</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Semi-Supervised Neural Text Generation by Joint Learning of NLU and NLG</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Sequence-to-Sequence Models for Data-to-Text Natural Language Generation - Word v Char and Out Diversity</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Summarization with Pointer-Generator Networks</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Table-to-Text Generation by Structure-Aware Seq2seq Learning</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>TabNet Attentive Interpretable Tabular Learning</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>Text-to-Text Pre-Training for Data-to-Text Tasks</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>The E2E Dataset New Challenges For End-to-End Generation</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper><paper><paper_name>ToTTo A Controlled Table-To-Text Generation Dataset</paper_name><task></task><data></data><contribution></contribution><arch></arch><improv></improv></paper>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Papers/out1.xml\", \"w\") as file:\n",
    "    file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
