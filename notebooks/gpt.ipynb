{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0db614cba24ebfa7082d81ed711ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb2da8e921e4a2fa4e3ef278db7b5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd6ed7fed1140a5a93ab18fabc8d964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f080d28951047bea795c2707faaf7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "generated = tokenizer.encode(\"The Manhattan bridge\")\n",
    "context = torch.tensor([generated])\n",
    "past = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "The Manhattan bridge is a major artery for the city's subway system, and the bridge is one of the busiest in the country.\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge is one of the busiest in the country.\n",
      "\n",
      "The bridge\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    #print(i)\n",
    "    output, past = model(context, past=past)\n",
    "    print(len(past))\n",
    "    token = torch.argmax(output[..., -1, :])\n",
    "\n",
    "    generated += [token.tolist()]\n",
    "    context = token.unsqueeze(0)\n",
    "\n",
    "sequence = tokenizer.decode(generated)\n",
    "\n",
    "print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2 import GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2()\n",
    "model_dict = model.state_dict() #currently with random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h.0.attn.bias',\n",
       " 'h.0.attn.c_attn.bias',\n",
       " 'h.0.attn.c_attn.weight',\n",
       " 'h.0.attn.c_proj.bias',\n",
       " 'h.0.attn.c_proj.weight',\n",
       " 'h.0.feedforward.c_fc.bias',\n",
       " 'h.0.feedforward.c_fc.weight',\n",
       " 'h.0.feedforward.c_proj.bias',\n",
       " 'h.0.feedforward.c_proj.weight',\n",
       " 'h.0.ln_1.bias',\n",
       " 'h.0.ln_1.weight',\n",
       " 'h.0.ln_2.bias',\n",
       " 'h.0.ln_2.weight',\n",
       " 'h.1.attn.bias',\n",
       " 'h.1.attn.c_attn.bias',\n",
       " 'h.1.attn.c_attn.weight',\n",
       " 'h.1.attn.c_proj.bias',\n",
       " 'h.1.attn.c_proj.weight',\n",
       " 'h.1.feedforward.c_fc.bias',\n",
       " 'h.1.feedforward.c_fc.weight',\n",
       " 'h.1.feedforward.c_proj.bias',\n",
       " 'h.1.feedforward.c_proj.weight',\n",
       " 'h.1.ln_1.bias',\n",
       " 'h.1.ln_1.weight',\n",
       " 'h.1.ln_2.bias',\n",
       " 'h.1.ln_2.weight',\n",
       " 'h.10.attn.bias',\n",
       " 'h.10.attn.c_attn.bias',\n",
       " 'h.10.attn.c_attn.weight',\n",
       " 'h.10.attn.c_proj.bias',\n",
       " 'h.10.attn.c_proj.weight',\n",
       " 'h.10.feedforward.c_fc.bias',\n",
       " 'h.10.feedforward.c_fc.weight',\n",
       " 'h.10.feedforward.c_proj.bias',\n",
       " 'h.10.feedforward.c_proj.weight',\n",
       " 'h.10.ln_1.bias',\n",
       " 'h.10.ln_1.weight',\n",
       " 'h.10.ln_2.bias',\n",
       " 'h.10.ln_2.weight',\n",
       " 'h.11.attn.bias',\n",
       " 'h.11.attn.c_attn.bias',\n",
       " 'h.11.attn.c_attn.weight',\n",
       " 'h.11.attn.c_proj.bias',\n",
       " 'h.11.attn.c_proj.weight',\n",
       " 'h.11.feedforward.c_fc.bias',\n",
       " 'h.11.feedforward.c_fc.weight',\n",
       " 'h.11.feedforward.c_proj.bias',\n",
       " 'h.11.feedforward.c_proj.weight',\n",
       " 'h.11.ln_1.bias',\n",
       " 'h.11.ln_1.weight',\n",
       " 'h.11.ln_2.bias',\n",
       " 'h.11.ln_2.weight',\n",
       " 'h.2.attn.bias',\n",
       " 'h.2.attn.c_attn.bias',\n",
       " 'h.2.attn.c_attn.weight',\n",
       " 'h.2.attn.c_proj.bias',\n",
       " 'h.2.attn.c_proj.weight',\n",
       " 'h.2.feedforward.c_fc.bias',\n",
       " 'h.2.feedforward.c_fc.weight',\n",
       " 'h.2.feedforward.c_proj.bias',\n",
       " 'h.2.feedforward.c_proj.weight',\n",
       " 'h.2.ln_1.bias',\n",
       " 'h.2.ln_1.weight',\n",
       " 'h.2.ln_2.bias',\n",
       " 'h.2.ln_2.weight',\n",
       " 'h.3.attn.bias',\n",
       " 'h.3.attn.c_attn.bias',\n",
       " 'h.3.attn.c_attn.weight',\n",
       " 'h.3.attn.c_proj.bias',\n",
       " 'h.3.attn.c_proj.weight',\n",
       " 'h.3.feedforward.c_fc.bias',\n",
       " 'h.3.feedforward.c_fc.weight',\n",
       " 'h.3.feedforward.c_proj.bias',\n",
       " 'h.3.feedforward.c_proj.weight',\n",
       " 'h.3.ln_1.bias',\n",
       " 'h.3.ln_1.weight',\n",
       " 'h.3.ln_2.bias',\n",
       " 'h.3.ln_2.weight',\n",
       " 'h.4.attn.bias',\n",
       " 'h.4.attn.c_attn.bias',\n",
       " 'h.4.attn.c_attn.weight',\n",
       " 'h.4.attn.c_proj.bias',\n",
       " 'h.4.attn.c_proj.weight',\n",
       " 'h.4.feedforward.c_fc.bias',\n",
       " 'h.4.feedforward.c_fc.weight',\n",
       " 'h.4.feedforward.c_proj.bias',\n",
       " 'h.4.feedforward.c_proj.weight',\n",
       " 'h.4.ln_1.bias',\n",
       " 'h.4.ln_1.weight',\n",
       " 'h.4.ln_2.bias',\n",
       " 'h.4.ln_2.weight',\n",
       " 'h.5.attn.bias',\n",
       " 'h.5.attn.c_attn.bias',\n",
       " 'h.5.attn.c_attn.weight',\n",
       " 'h.5.attn.c_proj.bias',\n",
       " 'h.5.attn.c_proj.weight',\n",
       " 'h.5.feedforward.c_fc.bias',\n",
       " 'h.5.feedforward.c_fc.weight',\n",
       " 'h.5.feedforward.c_proj.bias',\n",
       " 'h.5.feedforward.c_proj.weight',\n",
       " 'h.5.ln_1.bias',\n",
       " 'h.5.ln_1.weight',\n",
       " 'h.5.ln_2.bias',\n",
       " 'h.5.ln_2.weight',\n",
       " 'h.6.attn.bias',\n",
       " 'h.6.attn.c_attn.bias',\n",
       " 'h.6.attn.c_attn.weight',\n",
       " 'h.6.attn.c_proj.bias',\n",
       " 'h.6.attn.c_proj.weight',\n",
       " 'h.6.feedforward.c_fc.bias',\n",
       " 'h.6.feedforward.c_fc.weight',\n",
       " 'h.6.feedforward.c_proj.bias',\n",
       " 'h.6.feedforward.c_proj.weight',\n",
       " 'h.6.ln_1.bias',\n",
       " 'h.6.ln_1.weight',\n",
       " 'h.6.ln_2.bias',\n",
       " 'h.6.ln_2.weight',\n",
       " 'h.7.attn.bias',\n",
       " 'h.7.attn.c_attn.bias',\n",
       " 'h.7.attn.c_attn.weight',\n",
       " 'h.7.attn.c_proj.bias',\n",
       " 'h.7.attn.c_proj.weight',\n",
       " 'h.7.feedforward.c_fc.bias',\n",
       " 'h.7.feedforward.c_fc.weight',\n",
       " 'h.7.feedforward.c_proj.bias',\n",
       " 'h.7.feedforward.c_proj.weight',\n",
       " 'h.7.ln_1.bias',\n",
       " 'h.7.ln_1.weight',\n",
       " 'h.7.ln_2.bias',\n",
       " 'h.7.ln_2.weight',\n",
       " 'h.8.attn.bias',\n",
       " 'h.8.attn.c_attn.bias',\n",
       " 'h.8.attn.c_attn.weight',\n",
       " 'h.8.attn.c_proj.bias',\n",
       " 'h.8.attn.c_proj.weight',\n",
       " 'h.8.feedforward.c_fc.bias',\n",
       " 'h.8.feedforward.c_fc.weight',\n",
       " 'h.8.feedforward.c_proj.bias',\n",
       " 'h.8.feedforward.c_proj.weight',\n",
       " 'h.8.ln_1.bias',\n",
       " 'h.8.ln_1.weight',\n",
       " 'h.8.ln_2.bias',\n",
       " 'h.8.ln_2.weight',\n",
       " 'h.9.attn.bias',\n",
       " 'h.9.attn.c_attn.bias',\n",
       " 'h.9.attn.c_attn.weight',\n",
       " 'h.9.attn.c_proj.bias',\n",
       " 'h.9.attn.c_proj.weight',\n",
       " 'h.9.feedforward.c_fc.bias',\n",
       " 'h.9.feedforward.c_fc.weight',\n",
       " 'h.9.feedforward.c_proj.bias',\n",
       " 'h.9.feedforward.c_proj.weight',\n",
       " 'h.9.ln_1.bias',\n",
       " 'h.9.ln_1.weight',\n",
       " 'h.9.ln_2.bias',\n",
       " 'h.9.ln_2.weight',\n",
       " 'ln_f.bias',\n",
       " 'ln_f.weight',\n",
       " 'out.weight',\n",
       " 'wpe.weight',\n",
       " 'wte.weight']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(model_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 h.0.ln_2.bias\n",
      "12 h.0.ln_2.weight\n",
      "24 h.1.ln_2.bias\n",
      "25 h.1.ln_2.weight\n",
      "37 h.10.ln_2.bias\n",
      "38 h.10.ln_2.weight\n",
      "50 h.11.ln_2.bias\n",
      "51 h.11.ln_2.weight\n",
      "63 h.2.ln_2.bias\n",
      "64 h.2.ln_2.weight\n",
      "76 h.3.ln_2.bias\n",
      "77 h.3.ln_2.weight\n",
      "89 h.4.ln_2.bias\n",
      "90 h.4.ln_2.weight\n",
      "102 h.5.ln_2.bias\n",
      "103 h.5.ln_2.weight\n",
      "115 h.6.ln_2.bias\n",
      "116 h.6.ln_2.weight\n",
      "128 h.7.ln_2.bias\n",
      "129 h.7.ln_2.weight\n",
      "141 h.8.ln_2.bias\n",
      "142 h.8.ln_2.weight\n",
      "154 h.9.ln_2.bias\n",
      "155 h.9.ln_2.weight\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(sorted(list(model_dict.keys()))):\n",
    "    if \"ln_2\" in key:\n",
    "        print(i, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../anno-gpt/gpt2-pytorch_model.bin\") #pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 h.0.ln_2.bias\n",
      "12 h.0.ln_2.weight\n",
      "24 h.1.ln_2.bias\n",
      "25 h.1.ln_2.weight\n",
      "37 h.10.ln_2.bias\n",
      "38 h.10.ln_2.weight\n",
      "50 h.11.ln_2.bias\n",
      "51 h.11.ln_2.weight\n",
      "63 h.2.ln_2.bias\n",
      "64 h.2.ln_2.weight\n",
      "76 h.3.ln_2.bias\n",
      "77 h.3.ln_2.weight\n",
      "89 h.4.ln_2.bias\n",
      "90 h.4.ln_2.weight\n",
      "102 h.5.ln_2.bias\n",
      "103 h.5.ln_2.weight\n",
      "115 h.6.ln_2.bias\n",
      "116 h.6.ln_2.weight\n",
      "128 h.7.ln_2.bias\n",
      "129 h.7.ln_2.weight\n",
      "141 h.8.ln_2.bias\n",
      "142 h.8.ln_2.weight\n",
      "154 h.9.ln_2.bias\n",
      "155 h.9.ln_2.weight\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(sorted(list(state_dict.keys()))):\n",
    "    if \"ln_2\" in key:\n",
    "        print(i, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-68c8f208d954>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpretrained_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# model_dict.update(pretrained_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-68c8f208d954>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mstate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpretrained_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# model_dict.update(pretrained_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_dict' is not defined"
     ]
    }
   ],
   "source": [
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys(): \n",
    "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
    "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
    "        new_keys.append(new_key)\n",
    "        old_keys.append(key)\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys): \n",
    "    state_dict[new_key]=state_dict.pop(old_key)\n",
    "\n",
    "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
    "\n",
    "# model_dict.update(pretrained_dict)\n",
    "# model.load_state_dict(model_dict)\n",
    "# model.eval() #model in inference mode as it's now initialized with pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
